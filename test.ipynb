{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis con IMDb Movie DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, capacitaremos un modelo de regresión logística simple para clasificar reseñas de películas a partir del conjunto de datos de revisión de IMDb de 50k que ha sido coleccionado por Maas et. Alabama.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "El dataset consiste de 50.000 reseñas de peliculas los directorios orginales carpetas \"train\" y \"test\". la clase de etiquetas son binarias (1=positivas y 0= negativas) y contiene 25,000 positivos y 25,000 negativos reseñas respectivamente. \n",
    "Bueno por simplicidad lo ensambe en una archivo de reseña tivo excel CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.head() # mostrar ultimo 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data\n",
    "\n",
    "Ahora vamos a definir un simple tokenizador que divide el texto en palabras individuales oseas \"tokens\", ademas,usaremos algunas expresiones regulares simples para eliminar el marcado HTML y todos los caracteres que no sean letras, como \"emoticonos\", tambien  convertir el texto a minúsculas, eliminar palabras clave y aplicar el algoritmo de derivación de Porter para convertir las palabras en su forma raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) #+ ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora vamos hacer un intento y mostrar algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'hoa']\n",
      "2\n",
      "[['x'], ['test'], ['hoa']]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(tokenizer('This :) is a <a> test!\"hoa\" :-)</br>'))\n",
    "\n",
    "#tamaño de la palabra\n",
    "print(len(tokenizer('This :) is a <a> test!\"hoa\" :-)</br>')))\n",
    "\n",
    "#convertir tokesn a lista de lista\n",
    "\n",
    "def toList(text):\n",
    "    review = []\n",
    "    for w in text:\n",
    "        review.append([w])\n",
    "    return review\n",
    "    \n",
    "print(toList(tokenizer('This x:) is a <a> test!\"hoa\" :-)</br>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pedro', 'cambio', 'xd', 'algoritmo', 'hoa']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('This :) is a <a> pedro cambio! xD algoritmo\"hoa\" :-)</br><>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento \n",
    "Primero vamos definir un modulo que retorne un text body y su correspondiente etiqueta, para ello vamos a implementar este modulo acontinuacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # saltar header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para confirmar que la funcion `stream_docs` recupera segun lo previsto, vamos a ejecutar el siguiente codigo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hayamos verificado que nuestras funcion `stream_docs` funciona, ahora implementaremos una función `get_minibatch`(pedazos) para obtener un número específico(size) de documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        \n",
    "        docs.append(toList(tokenizer(text)))\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# primero vamos cargar todos los datos en un X y Y para extraer las caracteristias \n",
    "\n",
    "X,Y = get_minibatch(stream_docs(path='shuffled_movie_data.csv'), size= 5000) # get string of text\n",
    "\n",
    "#print(ar[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: 5000\n",
      "size of X[4]:  62\n",
      "dato : [['recently'], ['bought'], ['dvd'], ['forgetting'], ['much'], ['hated'], ['movie'], ['version'], ['chorus'], ['line'], ['every'], ['change'], ['director'], ['attenborough'], ['made'], ['story'], ['failed'], ['making'], ['director'], ['cassie'], ['relationship'], ['prominent'], ['entire'], ['ensemble'], ['premise'], ['musical'], ['sails'], ['window'], ['musical'], ['numbers'], ['sped'], ['rushed'], ['show'], ['hit'], ['song'], ['gets'], ['entire'], ['meaning'], ['shattered'], ['given'], ['cassie'], ['character'], ['overall'], ['staging'], ['self'], ['conscious'], ['reason'], ['give'], ['2'], ['great'], ['numbers'], ['still'], ['able'], ['enjoyed'], ['despite'], ['film'], ['attempt'], ['squeeze'], ['every'], ['bit'], ['joy'], ['spontaneity']]\n"
     ]
    }
   ],
   "source": [
    "print('size of X:',len(X))\n",
    "print('size of X[4]: ',len(X[4]))\n",
    "print('dato :',X[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hasta este punto ya hemos hecho la extraccion de caracteristicas y alguna parte de tokenizacion y agrupamiento seleccion en pedazos de string de palabras en un arreglo\n",
    "\n",
    "### Word embedding o extraccion de caracteristicas para el Modelado \n",
    "\n",
    "Primero, vamos a ver que trabajar con texto es un poco complicado para el modelamiento con una red neuronal o una algoritmo de aprendizaje automatico, asi que transformar a un lenguage de numero o vectores de numero, pero viendo la relacion entre las palabras es algo realmente muy loco (genial), y esto suena demasiado complicado verdad?, pues si... en realidad transformar palabras a numero o vectores de numero de nxn dimensiones es to much\", pero afortunadamente existen `word2vect` que fue desarrollado por un googler mikolov tal y como dice aqui https://en.wikipedia.org/wiki/Word2vec, asi que no hay que preocuparnos por eso solo hay que usarlo y `gensim` libreria en python hara la magia por nosotros , bueno tambien debemos indicar que existen otros como glove, fastText.. etc\n",
    "\n",
    "#### embedding con gensim desde cero\n",
    "\n",
    "bueno, en es paso vamos usar la libreria Gensim para generar nuestro proppio diccionarion de un Word2vect embedding de tal forma que tambien podemos definir el tamaño de los vectores por palabra, para este ejemplo y por motivos de prueba solo consideramos 20 el tamaño de cada vector,sin embargo se recommienda de 100-300 para el optimo desempeño, lo que se hace aqui es:\n",
    "\n",
    "- generar nuestro modelo word2vect = n\n",
    "- almacenamos las palabras con fuerte relacion de vencidad en un diccionario = w2v\n",
    "- consideramos el promedio de los vectores para su mejor representacion de los features\n",
    "- finalmente se almacena en un X_total (total de features 'matriz' 50000 x 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir el numero de  el word embedding para cada review\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewilderd/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_total = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    n = gensim.models.Word2Vec(X[i], size= 20,min_count=1)\n",
    "    w2v = dict(zip(n.wv.index2word, n.wv.syn0))\n",
    "    letra = list(n.wv.vocab)\n",
    "    da = []\n",
    "    for i in range(len(w2v)):\n",
    "        da.append(w2v[letra[i]])\n",
    "    X_total.append(np.mean(da, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, haremos uso del \"truco de hash\" a través de scikit-learns HashingVectorizer para crear un modelo de bolsa de BOW para nuestras descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 20), (0,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#Definiremos el dato para el test\n",
    "X_train,X_test=X_total[:45000], X_total[45000:]\n",
    "X_test = np.asarray(X_test)\n",
    "X_train = np.asarray(X_train)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptando mis datos para la NN\n",
    "\n",
    "en esta parte adaptamos los datos para que se puedan operar de la forma que implemente la red neuronal de 3 capas , aqui se puede ver:\n",
    "\n",
    "x_train, X_test, Y_train, Y_test que son el dato para el training y el test, respectivamente con 45 mil y 5 mil para el test.\n",
    "tambien debemos decir que X_total , Y_total son la cantidad de features y labels totales 50 mil filas cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penultimo:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5000, 2), (0, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gensim\n",
    "# sea X la lista de textos token (es decir lista de lista de tokens)\n",
    "modelo = gensim.models.Word2Vec(X, size=20, workers= 4)\n",
    "w2v = dict(zip(modelo.wv.index2word, modelo.wv.syn0))\n",
    "# reducir el modelo en vocabulario de tamaño en el modelo\n",
    "words = list(modelo.wv.vocab)\n",
    "#guardaremos el modleo\n",
    "modelo.save('model.bin')\n",
    "#abrir el modelo y cargar\n",
    "new_model = gensim.models.Word2Vec.load('model.bin')\n",
    "\n",
    "print('words: ',len(words))\n",
    "'''\n",
    "\n",
    "X = np.asarray(X_train)\n",
    "#Y = np.asarray(Y)\n",
    "#Y_train = np.zeros((5000,2))\n",
    "y_1 = []\n",
    "y_2 = []\n",
    "#validar Y_train\n",
    "for k in range(len(Y)):\n",
    "    if Y[k] == 1:\n",
    "        y_1.append(Y[k])  \n",
    "        y_2.append(0)\n",
    "    else:\n",
    "        y_1.append(0)  \n",
    "        y_2.append(Y[k])\n",
    "Y_total = np.column_stack((np.array(y_1),np.array(y_2)))\n",
    "print('penultimo: ',Y_total[0][0])\n",
    "\n",
    "#dividir y\n",
    "Y_train = np.asarray(Y_total[:45000])\n",
    "Y_test = np.asarray(Y_total[45000:])\n",
    "        \n",
    "Y_train.shape , Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewilderd/anaconda3/envs/conda_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a4c7dd04c9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m45000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_h2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    255\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,20), random_state=1)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=300)\n",
    "classes = np.array([0, 1])\n",
    "clf.partial_fit(X_train, Y[:45000], classes=classes)\n",
    "y_h2 = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('SKLEARN NN accuracy of Test: ',accuracy_score(y_pred=y_h2,y_true=Y[45000:])*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
